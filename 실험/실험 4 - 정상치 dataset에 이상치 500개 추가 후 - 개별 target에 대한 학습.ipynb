{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15971,
     "status": "ok",
     "timestamp": 1661209966220,
     "user": {
      "displayName": "Jaewoo Choi",
      "userId": "05968598841946004036"
     },
     "user_tz": -540
    },
    "id": "h6h89BPK1CaA",
    "outputId": "4022c5d9-2fa8-4b1e-9545-7410a88b7913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35443,
     "status": "ok",
     "timestamp": 1661210001660,
     "user": {
      "displayName": "Jaewoo Choi",
      "userId": "05968598841946004036"
     },
     "user_tz": -540
    },
    "id": "V1s0gwOEkAYi",
    "outputId": "f807e119-a943-48cd-dd33-657e38379608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/Project/Dacon/lgauto/open.zip\n",
      "   creating: meta/\n",
      "  inflating: meta/x_feature_info.csv  \n",
      "  inflating: meta/y_feature_info.csv  \n",
      "  inflating: meta/y_feature_spec_info.csv  \n",
      "  inflating: sample_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.40)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 10.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "\u001b[K     |████████████████████████████████| 209 kB 72.7 MB/s \n",
      "\u001b[?25hCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 7.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.9.0)\n",
      "Collecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 90.1 MB/s \n",
      "\u001b[?25hCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 84.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 7.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=bed6673c988b0228c1bc0f6eecb542e6b4e4e16f35e0df2fead5790da8f3b63f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
      "Successfully installed Mako-1.2.1 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 optuna-2.10.1 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.6 MB 149 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-1.0.6\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting skranger\n",
      "  Downloading skranger-0.7.0-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (405 kB)\n",
      "\u001b[K     |████████████████████████████████| 405 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting scikit-learn<1,>=0.23.0\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 90.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.1.0)\n",
      "Installing collected packages: scikit-learn, skranger\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
      "Successfully installed scikit-learn-0.24.2 skranger-0.7.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ngboost\n",
      "  Downloading ngboost-0.3.12-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from ngboost) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from ngboost) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from ngboost) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from ngboost) (1.7.3)\n",
      "Collecting lifelines>=0.25\n",
      "  Downloading lifelines-0.27.1-py3-none-any.whl (349 kB)\n",
      "\u001b[K     |████████████████████████████████| 349 kB 7.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (3.2.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (1.3.5)\n",
      "Collecting autograd-gamma>=0.3\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "Collecting formulaic>=0.2.2\n",
      "  Downloading formulaic-0.4.0-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 6.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (1.4)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines>=0.25->ngboost) (0.16.0)\n",
      "Requirement already satisfied: cached_property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.5.2)\n",
      "Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (0.8.1)\n",
      "Collecting graphlib-backport<2.0.0,>=1.0.0\n",
      "  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.2.0\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n",
      "Collecting interface-meta<2.0.0,>=1.2.0\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->lifelines>=0.25->ngboost) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->ngboost) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->ngboost) (1.1.0)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=9103fbe1fbdd6d32390bff9126a57a61ab3de8cc74076f62a8898573fc5a1eee\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: typing-extensions, interface-meta, graphlib-backport, formulaic, autograd-gamma, lifelines, ngboost\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.1.0 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n",
      "spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed autograd-gamma-0.5.0 formulaic-0.4.0 graphlib-backport-1.0.3 interface-meta-1.3.0 lifelines-0.27.1 ngboost-0.3.12 typing-extensions-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!unzip /content/drive/MyDrive/Project/Dacon/lgauto/open.zip\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "# !cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n",
    "!pip install optuna\n",
    "!pip install catboost\n",
    "!pip install skranger\n",
    "!pip install ngboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pcG2aSVkkAVz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lasso, Ridge\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from skranger.ensemble import RangerForestRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8e67ifiDnv5w"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "  seed = 42\n",
    "  epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-ZK7vgIBkAS-"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V8XIRg1wkAQW"
   },
   "outputs": [],
   "source": [
    "def dataset_split_X_y(df):    \n",
    "    \"\"\"\n",
    "    @Description: split data into features and labels\n",
    "    @Param: df, pandas dataframe with columns starting with X for features and Y for labels\n",
    "    @Return: features and labels in pandas dataframes\n",
    "    \"\"\"\n",
    "    xs = df.filter(regex='X') # Input : X Feature\n",
    "    ys = df.filter(regex='Y') # Output : Y Feature\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KBgkm1KPlYtZ"
   },
   "outputs": [],
   "source": [
    "def check_for_NAs(df, show=False):\n",
    "    \"\"\"\n",
    "    @Description: checks for the NAs in the dataframe\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: show, boolean indicating whether NaN data are also necessary as a part of the output\n",
    "    @Return: name of the columns with NaN\n",
    "    \"\"\"\n",
    "    nan_values = df.loc[:, df.isnull().any()]\n",
    "    if show:\n",
    "        return df[df.isna().any(axis=1)]\n",
    "    return list(nan_values.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZgMxozATlbCp"
   },
   "outputs": [],
   "source": [
    "def check_for_label_bound(df, labels, bound):\n",
    "    \"\"\"\n",
    "    @Description: check bound is inbetween min and max\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: labels, list of column names \n",
    "    @Param3: thres: list of bounds\n",
    "    @Return: names of the columns not within the bound\n",
    "    \"\"\"\n",
    "    n = len(labels)\n",
    "    result = []\n",
    "    for idx in range(n):\n",
    "        col = labels[idx]\n",
    "        thres = bound[idx]\n",
    "        extracted_column = df[col]\n",
    "        if not extracted_column.between(thres[0], thres[1]).all():\n",
    "            result.append(labels[idx])\n",
    "    if len(result) == 0:\n",
    "        print('everything is within the bound')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fA_w1Rt-kADH"
   },
   "outputs": [],
   "source": [
    "def zero_variance(df):\n",
    "    \"\"\"\n",
    "    @Description: check for zero_variance\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Return: names of the columns with zero variance\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for col in df.columns:\n",
    "        if df[col].var() == 0:\n",
    "            result.append(col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NtEGKh0zlErb"
   },
   "outputs": [],
   "source": [
    "def get_top_correlation(df, n=10):\n",
    "    \"\"\"\n",
    "    @Description: print out top correlated features\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: n, number of lines to print \n",
    "    @Return: pandas series\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    for idx1 in range(0, df.shape[1]):\n",
    "        for idx2 in range(0, idx1+1):\n",
    "            pairs.add((df.columns[idx1], df.columns[idx2]))\n",
    "    corr = df.corr().abs().unstack()\n",
    "    corr = corr.drop(labels=pairs).sort_values(ascending=False)\n",
    "    return corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FlUrH-kLldrb"
   },
   "outputs": [],
   "source": [
    "def adjacent_histogram_boxplot(feature_var, figsize = (7, 5)):\n",
    "    \"\"\"\n",
    "    @Description: plot histogram and boxplot in next to each other\n",
    "    @Param1: feature_var, pandas series \n",
    "    @Param2: figsize, size of the figure \n",
    "    \"\"\"\n",
    "    fig, (hist_plot, box_plot) = plt.subplots(nrows=2, sharex=True, gridspec_kw={'height_ratios':(.85,.15)}, figsize=figsize)\n",
    "    sns.distplot(feature_var, kde=True, ax=hist_plot, kde_kws= {\"linewidth\":1.5}) \n",
    "    sns.boxplot(feature_var, ax=box_plot, linewidth = 1, width = 0.5)\n",
    "    hist_plot.set_ylabel('')    \n",
    "    hist_plot.set_xlabel('')\n",
    "    box_plot.set_xlabel('')\n",
    "    hist_plot.tick_params(labelsize=8)\n",
    "    box_plot.tick_params(labelsize=8)\n",
    "    fig.suptitle(feature_var.name, fontsize = 10)\n",
    "    hist_plot.axvline(np.mean(feature_var),color='red',linestyle='-',lw = 1.5)\n",
    "    hist_plot.axvline(np.median(feature_var),color='green',linestyle='--',lw = 1.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6CEAj3XQlEgR"
   },
   "outputs": [],
   "source": [
    "def lg_nrmse(gt, preds):\n",
    "    \"\"\"\n",
    "    @Description: Metric used in this project\n",
    "    @Params1: gt, pandas dataframe\n",
    "    @Param2: preds, pandas dataframe\n",
    "    @Return: nrmse score\n",
    "    \"\"\"\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    preds = pd.DataFrame(preds)\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14):\n",
    "        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HfB2waRKlEdS"
   },
   "outputs": [],
   "source": [
    "def lg_individual_nrmse(gt, preds):\n",
    "    \"\"\"\n",
    "    @Description: Metric used in this project (individual)\n",
    "    @Params1: gt, pandas dataframe\n",
    "    @Param2: preds, pandas dataframe\n",
    "    @Return: nrmse score\n",
    "    \"\"\"\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    rmse = mean_squared_error(gt, preds, squared=False)\n",
    "    nrmse = rmse/np.mean(np.abs(gt))\n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-zIBOcBUliiY"
   },
   "outputs": [],
   "source": [
    "def find_outlier_zscore(data, threshold = 3):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    zs = [(y - mean) / std for y in data]\n",
    "    masks = np.where(np.abs(zs) > threshold)\n",
    "    return masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QenNjbcYlkM5"
   },
   "outputs": [],
   "source": [
    "ys = ['Y_01', 'Y_02', 'Y_03', 'Y_04', 'Y_05', \n",
    "      'Y_06', 'Y_07', 'Y_08', 'Y_09', 'Y_10', \n",
    "      'Y_11', 'Y_12', 'Y_13', 'Y_14']\n",
    "ys_bounds = [[0.2, 2], [0.2, 2.1], [0.2, 2.1], \n",
    "             [7, 19], [22, 36.5], [-19.2, 19], \n",
    "             [2.4, 4], [-29.2, -24], [-29.2, -24],\n",
    "             [-30.6, -20], [19.6, 26.6], [-29.2, -24],\n",
    "             [-29.2, -24], [-29.2, -24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FB59rtt-llsC"
   },
   "outputs": [],
   "source": [
    "seed_everything(Config.seed)\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_x = pd.read_csv('data/test.csv')\n",
    "train_x, train_y = dataset_split_X_y(train_df)\n",
    "\n",
    "cols_with_zero_variance = zero_variance(train_x) # 분산이 0 (통과 여부)\n",
    "train_x = train_x.drop(cols_with_zero_variance, axis = 1)\n",
    "test_x = test_x.drop(cols_with_zero_variance, axis = 1)\n",
    "\n",
    "train_x = train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치가 많음 (결측치 = 0, 공지사항)\n",
    "test_x = test_x.drop(['X_10', 'X_11'], axis = 1)\n",
    "\n",
    "test_x = test_x.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator = pd.DataFrame()\n",
    "for idx in range(len(ys)):\n",
    "    y_series = ~train_y[ys[idx]].between(ys_bounds[idx][0], ys_bounds[idx][1])\n",
    "    df_indicator = pd.concat([df_indicator, y_series.astype(int)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3917\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for i in df_indicator.columns:\n",
    "    lst.append(df_indicator[df_indicator[i] == 1].index)\n",
    "ans=set()\n",
    "for i in lst:\n",
    "    for k in i:\n",
    "        ans.add(k)\n",
    "print(len(ans))\n",
    "\n",
    "ans = list(ans)\n",
    "ans.sort()\n",
    "spec_x = train_x.loc[ans, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_normal = train_x.drop(spec_x.index)\n",
    "train_y_normal = train_y.drop(spec_x.index)\n",
    "\n",
    "train_x_spec = train_x.drop(train_x_normal.index)\n",
    "train_y_spec = train_y.drop(train_y_normal.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3917"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uWxyzIilqLVx"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "  seed = 42\n",
    "  epochs = 200\n",
    "  cv=10\n",
    "  test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PhFxUQZhklLd"
   },
   "outputs": [],
   "source": [
    "def lgbm_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'min_child_samples': int(params['min_child_samples']),\n",
    "        'colsample_bytree': '{:.5f}'.format(params['colsample_bytree']),\n",
    "        'subsample': '{:.5f}'.format(params['subsample']),\n",
    "        'min_split_gain': '{:.5f}'.format(params['min_split_gain']),\n",
    "        'scale_pos_weight': '{:.5f}'.format(params['scale_pos_weight']),\n",
    "        'reg_alpha': '{:.5f}'.format(params['reg_alpha']),\n",
    "        'reg_lambda': '{:.5f}'.format(params['reg_lambda']),\n",
    "        'learning_rate': '{:.5f}'.format(params['learning_rate']),   \n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_jobs = -1,\n",
    "        random_state = 1,\n",
    "        verbose = 100,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x_normal, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    \n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "def xgb_objective(params):\n",
    "    params = {\n",
    "\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_jobs = -1,\n",
    "        verbose = 100,\n",
    "        random_state = 1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "def cat_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'depth': int(params['depth']),\n",
    "        'learning_rate': params['learning_rate'],   \n",
    "        'l2_leaf_reg': params['l2_leaf_reg'],\n",
    "        'max_bin': int(params['max_bin']),\n",
    "        'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "        'random_strength': params['random_strength'],\n",
    "        'fold_len_multiplier': params['fold_len_multiplier'],\n",
    "        \n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        logging_level='Silent',\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_04'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_04']))\n",
    "    return losses.mean()\n",
    "\n",
    "def extra_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_split': int(params['min_samples_split']),\n",
    "        'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "        'min_weight_fraction_leaf': params['min_weight_fraction_leaf'],\n",
    "        'max_features': params['max_features'],\n",
    "        'max_leaf_nodes': int(params['max_leaf_nodes']),\n",
    "        'min_impurity_decrease': params['min_impurity_decrease'],\n",
    "        'bootstrap': params['bootstrap'],\n",
    "        'ccp_alpha': params['ccp_alpha'],  \n",
    "    }\n",
    "\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_jobs = -1,\n",
    "        verbose = 0,\n",
    "        random_state = 1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    return losses.mean()\n",
    "\n",
    "def ngbr_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'natural_gradient': params['natural_gradient'],\n",
    "        'col_sample': float(params['col_sample']),\n",
    "        'minibatch_frac': float(params['minibatch_frac']),\n",
    "        'tol': float(params['tol']),\n",
    "    }\n",
    "\n",
    "    model = NGBRegressor(\n",
    "        verbose = 100,\n",
    "        random_state = 1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJEAijvRaiPb"
   },
   "source": [
    "## Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlT0GKmqaj_H",
    "outputId": "f62ed8e1-48f5-4977-d773-c5d745a4f047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [35:40<00:00, 10.70s/it, best loss: 0.1916584045864259]\n"
     ]
    }
   ],
   "source": [
    "## https://catboost.ai/en/docs/concepts/parameter-tuning (참고)\n",
    "space_catboost = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 300, 50),\n",
    "    'depth': hp.quniform(\"depth\", 2, 16, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n",
    "    'max_bin' : hp.quniform('max_bin', 1, 254, 1),\n",
    "    'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 2, 700, 1),\n",
    "    'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "    'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = cat_objective,\n",
    "            space = space_catboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33Rrmodh1OpG"
   },
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z42ZYLiqFDSV"
   },
   "source": [
    "## Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfgLLU2OFKg0"
   },
   "outputs": [],
   "source": [
    "space_extra = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 50),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 50, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 5, 50, 5),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 50, 1),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.01, 0.5),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None, 'auto']),\n",
    "    'max_leaf_nodes': hp.quniform('max_leaf_nodes', 3, 30, 1),\n",
    "    'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 200),\n",
    "    'bootstrap':  hp.choice('bootstrap', [True, False]),\n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.01, 1.0),\n",
    "}\n",
    "\n",
    "best = fmin(fn = extra_objective,\n",
    "            space = space_extra,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 2)\n",
    "\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['max_leaf_nodes'] = int(best['max_leaf_nodes'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6_7kTkbVZJh"
   },
   "source": [
    "##LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9E4wigN9rTR"
   },
   "outputs": [],
   "source": [
    "space_lgbm = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 250, 1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 200, 5),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 10, 150, 5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'subsample': hp.uniform('subsample', 0.3, 1.0),\n",
    "    'min_split_gain': hp.uniform('min_split_gain', 0, 0.7),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 500),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 500),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = lgbm_objective,\n",
    "            space = space_lgbm,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 200)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [35690, 39607]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m best \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.572280100273023\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.010283635038627429\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m180\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m135\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_split_gain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.04511227284338413\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m900\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m70\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4.406681827912319\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20.4785600448913\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_pos_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8.302374117433086\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1688669888026464\u001b[39m}\n\u001b[1;32m----> 4\u001b[0m val2 \u001b[38;5;241m=\u001b[39m \u001b[43mlgbm_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mlgbm_objective\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]),   \n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m LGBMRegressor(\n\u001b[0;32m     17\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     19\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39m\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_x_normal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY_01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m losses \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_01\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNRMSE Loss \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m params \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(losses\u001b[38;5;241m.\u001b[39mmean(), params))\n",
      "File \u001b[1;32mD:\\ai\\envs\\dacon\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\ai\\envs\\dacon\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:252\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_validate\u001b[39m(\n\u001b[0;32m     50\u001b[0m     estimator,\n\u001b[0;32m     51\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     64\u001b[0m ):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     cv \u001b[38;5;241m=\u001b[39m check_cv(cv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32mD:\\ai\\envs\\dacon\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 433\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\ai\\envs\\dacon\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [35690, 39607]"
     ]
    }
   ],
   "source": [
    "best = {'colsample_bytree': 0.572280100273023, 'learning_rate': 0.010283635038627429, 'max_depth': 180, 'min_child_samples': 135, 'min_split_gain': 0.04511227284338413, 'n_estimators': 900, 'num_leaves': 70, 'reg_alpha': 4.406681827912319, 'reg_lambda': 20.4785600448913, 'scale_pos_weight': 8.302374117433086, 'subsample': 0.1688669888026464}\n",
    "\n",
    "\n",
    "val2 = lgbm_objective(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = ['Y_01', 'Y_02', 'Y_03', 'Y_04', 'Y_05', \n",
    "      'Y_06', 'Y_07', 'Y_08', 'Y_09', 'Y_10', \n",
    "      'Y_11', 'Y_12', 'Y_13', 'Y_14']\n",
    "ys_bounds = [[0.2, 2], [0.2, 2.1], [0.2, 2.1], \n",
    "             [7, 19], [22, 36.5], [-19.2, 19], \n",
    "             [2.4, 4], [-29.2, -24], [-29.2, -24],\n",
    "             [-30.6, -20], [19.6, 26.6], [-29.2, -24],\n",
    "             [-29.2, -24], [-29.2, -24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22270174567825513"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator = pd.DataFrame()\n",
    "for idx in range(len(ys)):\n",
    "    y_series = ~train_y[ys[idx]].between(ys_bounds[idx][0], ys_bounds[idx][1])\n",
    "    df_indicator = pd.concat([df_indicator, y_series.astype(int)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3917\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for i in df_indicator.columns:\n",
    "    lst.append(df_indicator[df_indicator[i] == 1].index)\n",
    "ans=set()\n",
    "for i in lst:\n",
    "    for k in i:\n",
    "        ans.add(k)\n",
    "print(len(ans))\n",
    "\n",
    "ans = list(ans)\n",
    "ans.sort()\n",
    "spec_x = train_x.loc[ans, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_spec_x = pd.DataFrame([0 for i in range(len(train_x))])\n",
    "lst_spec_x.loc[spec_x.index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_normal = train_x.drop(spec_x.index)\n",
    "train_y_normal = train_y.drop(spec_x.index)\n",
    "\n",
    "train_x_spec = train_x.drop(train_x_normal.index)\n",
    "train_y_spec = train_y.drop(train_y_normal.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_end = pd.concat([train_x_spec.loc[:4000], train_x_normal])\n",
    "train_y_end = pd.concat([train_y_spec.loc[:4000], train_y_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x_spec.loc[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(best, y_val):\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "            n_jobs = -1,\n",
    "            random_state = 1,\n",
    "            **best\n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(train_x_normal, train_y_normal[y_val])\n",
    "    preds = model.predict(test_x)\n",
    "\n",
    "    return pd.DataFrame(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = func(best, 'Y_01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.376983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.447808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.379149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.411775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.306533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>1.273483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>1.297378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>1.249871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>1.241047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>1.334439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39608 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      1.376983\n",
       "1      1.447808\n",
       "2      1.379149\n",
       "3      1.411775\n",
       "4      1.306533\n",
       "...         ...\n",
       "39603  1.273483\n",
       "39604  1.297378\n",
       "39605  1.249871\n",
       "39606  1.241047\n",
       "39607  1.334439\n",
       "\n",
       "[39608 rows x 1 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'colsample_bytree': 0.572280100273023, 'learning_rate': 0.010283635038627429, 'max_depth': 180, 'min_child_samples': 135, 'min_split_gain': 0.04511227284338413, 'n_estimators': 900, 'num_leaves': 70, 'reg_alpha': 4.406681827912319, 'reg_lambda': 20.4785600448913, 'scale_pos_weight': 8.302374117433086, 'subsample': 0.1688669888026464}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_01')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.7641322280477741, 'learning_rate': 0.010977205425053654, 'max_depth': 90, 'min_child_samples': 75, 'min_split_gain': 0.13379952895779884, 'n_estimators': 900, 'num_leaves': 80, 'reg_alpha': 1.9214119194170154, 'reg_lambda': 14.454450236504218, 'scale_pos_weight': 2.171961031806387, 'subsample': 0.9552593593877317}\n",
    "\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_02')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.5504769098255781,  'learning_rate': 0.019653385015120244, 'max_depth': 220, 'min_child_samples': 25, 'min_split_gain': 0.1273611040963466, 'n_estimators': 470, 'num_leaves': 160, 'reg_alpha': 3.5549669150756706, 'reg_lambda': 39.88636182674132, 'scale_pos_weight': 12.46696320152359, 'subsample': 0.7590007450921917}\n",
    "\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_03')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.5597537952569402, 'learning_rate': 0.02374663979814546, 'max_depth': 32, 'min_child_samples': 100, 'min_split_gain': 0.12211426885216736, 'n_estimators': 1263, 'num_leaves': 200, 'reg_alpha': 14.606693962963451, 'reg_lambda': 299.52278825209424, 'scale_pos_weight': 7.7785016838070735, 'subsample': 0.6254745287838821}\n",
    "\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_04')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.4311015575880258, 'learning_rate': 0.01749725932551278, 'max_depth': 53, 'min_child_samples': 15, 'min_split_gain': 0.2820951740673634, 'n_estimators': 974, 'num_leaves': 165, 'reg_alpha': 9.604623064885754, 'reg_lambda': 12.314490508636432, 'scale_pos_weight': 6.6422956907936825, 'subsample': 0.7390190399971659}\n",
    "\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_05')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.6889745043181079, 'learning_rate': 0.06146161938790444, 'max_depth': 89, 'min_child_samples': 10, 'min_split_gain': 0.669592868575692, 'n_estimators': 1169, 'num_leaves': 175, 'reg_alpha': 11.405277636150856, 'reg_lambda': 112.37954230084294, 'scale_pos_weight': 5.932435783263877, 'subsample': 0.8265223228903998}  \n",
    "\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_06')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.8663251864650988, 'learning_rate': 0.018110306887688978, 'max_depth': 166, 'min_child_samples': 50, 'min_split_gain': 0.025403061552667243, 'n_estimators': 1080, 'num_leaves': 100, 'reg_alpha': 2.0131018839563666, 'reg_lambda': 63.56640846106552, 'scale_pos_weight': 1.8584564419776715, 'subsample': 0.7643028435523616}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_07')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.8970390757241629, 'learning_rate': 0.03571726260659087, 'max_depth': 164, 'min_child_samples': 30, 'min_split_gain': 0.2863362850926679, 'n_estimators': 740, 'num_leaves': 100, 'reg_alpha': 1.1167159754886287, 'reg_lambda': 280.9798636389436, 'scale_pos_weight': 4.75867892931176, 'subsample': 0.681716202670263}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_08')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'n_estimators': 900, 'max_depth': 86, 'num_leaves': 150, 'min_child_samples': 85, 'colsample_bytree': '0.90507', 'subsample': '0.62362', 'min_split_gain': '0.21034', 'scale_pos_weight': '8.77311', 'reg_alpha': '0.07069', 'reg_lambda': '499.10672', 'learning_rate': '0.04679'}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_09')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.8350973419202665, 'learning_rate': 0.03134966396365972, 'max_depth': 114, 'min_child_samples': 20, 'min_split_gain': 0.24406788869557822, 'n_estimators': 454, 'num_leaves': 115, 'reg_alpha': 1.0870546166564243, 'reg_lambda': 346.21163772786895, 'scale_pos_weight': 5.81617865285278, 'subsample': 0.45612075761336973}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_10')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.7285829045071064, 'learning_rate': 0.019839273085108612, 'max_depth': 71, 'min_child_samples': 50, 'min_split_gain': 0.35567737788276876, 'n_estimators': 970, 'num_leaves': 140, 'reg_alpha': 0.27353134227182774, 'reg_lambda': 157.85749037224548, 'scale_pos_weight': 5.956126991298146, 'subsample': 0.7509931500532172}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_11')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.6115826698158419, 'learning_rate': 0.010052927231718068, 'max_depth': 71, 'min_child_samples': 85, 'min_split_gain': 0.12003011548878659, 'n_estimators': 1300, 'num_leaves': 120, 'reg_alpha': 1.3013867029804251, 'reg_lambda': 269.3915696845848, 'scale_pos_weight': 5.290961082236748, 'subsample': 0.7542724715058367}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_12')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.9511047907962863, 'learning_rate': 0.023257873709858216, 'max_depth': 58, 'min_child_samples': 80, 'min_split_gain': 0.21488153574891886, 'n_estimators': 1300, 'num_leaves': 150, 'reg_alpha': 0.33761852089148814, 'reg_lambda': 57.05291849099506, 'scale_pos_weight': 2.0801436555772854, 'subsample': 0.5580106548214563}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_13')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "best =  {'colsample_bytree': 0.8851122740930837, 'learning_rate': 0.013136814152245062, 'max_depth': 249, 'min_child_samples': 65, 'min_split_gain': 0.2072264172906347, 'n_estimators': 450, 'num_leaves': 135, 'reg_alpha': 0.642890771203696, 'reg_lambda': 45.624663648443345, 'scale_pos_weight': 6.400746088779947, 'subsample': 0.30084274480143686}\n",
    "end_pd = pd.concat([end_pd, func(best, 'Y_14')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.391697</td>\n",
       "      <td>1.154704</td>\n",
       "      <td>1.033526</td>\n",
       "      <td>14.465251</td>\n",
       "      <td>32.407712</td>\n",
       "      <td>16.742986</td>\n",
       "      <td>3.012424</td>\n",
       "      <td>-26.034182</td>\n",
       "      <td>-26.024085</td>\n",
       "      <td>-22.127878</td>\n",
       "      <td>24.553438</td>\n",
       "      <td>-25.990061</td>\n",
       "      <td>-25.936504</td>\n",
       "      <td>-25.961329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.455883</td>\n",
       "      <td>1.181129</td>\n",
       "      <td>1.103510</td>\n",
       "      <td>13.448666</td>\n",
       "      <td>31.392519</td>\n",
       "      <td>16.695410</td>\n",
       "      <td>3.129448</td>\n",
       "      <td>-26.070582</td>\n",
       "      <td>-26.139123</td>\n",
       "      <td>-22.327433</td>\n",
       "      <td>24.402505</td>\n",
       "      <td>-26.038111</td>\n",
       "      <td>-26.120345</td>\n",
       "      <td>-26.043134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.375828</td>\n",
       "      <td>1.096432</td>\n",
       "      <td>1.011833</td>\n",
       "      <td>14.629861</td>\n",
       "      <td>31.759342</td>\n",
       "      <td>16.852628</td>\n",
       "      <td>3.095265</td>\n",
       "      <td>-25.978228</td>\n",
       "      <td>-25.872189</td>\n",
       "      <td>-22.060548</td>\n",
       "      <td>24.737351</td>\n",
       "      <td>-25.836653</td>\n",
       "      <td>-25.801612</td>\n",
       "      <td>-25.885519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.417349</td>\n",
       "      <td>1.148919</td>\n",
       "      <td>1.001002</td>\n",
       "      <td>15.230288</td>\n",
       "      <td>32.158597</td>\n",
       "      <td>17.151581</td>\n",
       "      <td>3.059559</td>\n",
       "      <td>-25.593811</td>\n",
       "      <td>-25.608469</td>\n",
       "      <td>-21.691326</td>\n",
       "      <td>25.014891</td>\n",
       "      <td>-25.569533</td>\n",
       "      <td>-25.607190</td>\n",
       "      <td>-25.615177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.307132</td>\n",
       "      <td>1.012789</td>\n",
       "      <td>0.936047</td>\n",
       "      <td>15.011538</td>\n",
       "      <td>31.435335</td>\n",
       "      <td>16.969695</td>\n",
       "      <td>3.020458</td>\n",
       "      <td>-25.688307</td>\n",
       "      <td>-25.677593</td>\n",
       "      <td>-21.883398</td>\n",
       "      <td>24.846907</td>\n",
       "      <td>-25.595199</td>\n",
       "      <td>-25.622568</td>\n",
       "      <td>-25.694433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>1.284618</td>\n",
       "      <td>1.011609</td>\n",
       "      <td>1.020121</td>\n",
       "      <td>12.481210</td>\n",
       "      <td>30.706559</td>\n",
       "      <td>16.776067</td>\n",
       "      <td>3.186461</td>\n",
       "      <td>-26.345256</td>\n",
       "      <td>-26.365513</td>\n",
       "      <td>-22.692625</td>\n",
       "      <td>24.471625</td>\n",
       "      <td>-26.314099</td>\n",
       "      <td>-26.272776</td>\n",
       "      <td>-26.287771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>1.278071</td>\n",
       "      <td>0.909527</td>\n",
       "      <td>0.981409</td>\n",
       "      <td>14.619227</td>\n",
       "      <td>31.650746</td>\n",
       "      <td>16.756380</td>\n",
       "      <td>3.149365</td>\n",
       "      <td>-26.304200</td>\n",
       "      <td>-26.287231</td>\n",
       "      <td>-22.669750</td>\n",
       "      <td>24.486648</td>\n",
       "      <td>-26.230034</td>\n",
       "      <td>-26.248674</td>\n",
       "      <td>-26.273253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>1.283944</td>\n",
       "      <td>0.944038</td>\n",
       "      <td>0.964451</td>\n",
       "      <td>13.256931</td>\n",
       "      <td>31.108957</td>\n",
       "      <td>16.735006</td>\n",
       "      <td>3.136356</td>\n",
       "      <td>-26.438000</td>\n",
       "      <td>-26.375102</td>\n",
       "      <td>-22.712396</td>\n",
       "      <td>24.377857</td>\n",
       "      <td>-26.406290</td>\n",
       "      <td>-26.480849</td>\n",
       "      <td>-26.499461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>1.231212</td>\n",
       "      <td>0.885186</td>\n",
       "      <td>0.949269</td>\n",
       "      <td>13.759728</td>\n",
       "      <td>31.372014</td>\n",
       "      <td>16.849927</td>\n",
       "      <td>3.157621</td>\n",
       "      <td>-26.385419</td>\n",
       "      <td>-26.321913</td>\n",
       "      <td>-22.713094</td>\n",
       "      <td>24.681208</td>\n",
       "      <td>-26.260347</td>\n",
       "      <td>-26.268088</td>\n",
       "      <td>-26.305381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>1.324696</td>\n",
       "      <td>0.995121</td>\n",
       "      <td>1.000797</td>\n",
       "      <td>12.910102</td>\n",
       "      <td>30.893806</td>\n",
       "      <td>16.697747</td>\n",
       "      <td>3.180815</td>\n",
       "      <td>-26.464506</td>\n",
       "      <td>-26.480959</td>\n",
       "      <td>-22.781458</td>\n",
       "      <td>24.374134</td>\n",
       "      <td>-26.393409</td>\n",
       "      <td>-26.470877</td>\n",
       "      <td>-26.445254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39608 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         0         0          0          0          0  \\\n",
       "0      1.391697  1.154704  1.033526  14.465251  32.407712  16.742986   \n",
       "1      1.455883  1.181129  1.103510  13.448666  31.392519  16.695410   \n",
       "2      1.375828  1.096432  1.011833  14.629861  31.759342  16.852628   \n",
       "3      1.417349  1.148919  1.001002  15.230288  32.158597  17.151581   \n",
       "4      1.307132  1.012789  0.936047  15.011538  31.435335  16.969695   \n",
       "...         ...       ...       ...        ...        ...        ...   \n",
       "39603  1.284618  1.011609  1.020121  12.481210  30.706559  16.776067   \n",
       "39604  1.278071  0.909527  0.981409  14.619227  31.650746  16.756380   \n",
       "39605  1.283944  0.944038  0.964451  13.256931  31.108957  16.735006   \n",
       "39606  1.231212  0.885186  0.949269  13.759728  31.372014  16.849927   \n",
       "39607  1.324696  0.995121  1.000797  12.910102  30.893806  16.697747   \n",
       "\n",
       "              0          0          0          0          0          0  \\\n",
       "0      3.012424 -26.034182 -26.024085 -22.127878  24.553438 -25.990061   \n",
       "1      3.129448 -26.070582 -26.139123 -22.327433  24.402505 -26.038111   \n",
       "2      3.095265 -25.978228 -25.872189 -22.060548  24.737351 -25.836653   \n",
       "3      3.059559 -25.593811 -25.608469 -21.691326  25.014891 -25.569533   \n",
       "4      3.020458 -25.688307 -25.677593 -21.883398  24.846907 -25.595199   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "39603  3.186461 -26.345256 -26.365513 -22.692625  24.471625 -26.314099   \n",
       "39604  3.149365 -26.304200 -26.287231 -22.669750  24.486648 -26.230034   \n",
       "39605  3.136356 -26.438000 -26.375102 -22.712396  24.377857 -26.406290   \n",
       "39606  3.157621 -26.385419 -26.321913 -22.713094  24.681208 -26.260347   \n",
       "39607  3.180815 -26.464506 -26.480959 -22.781458  24.374134 -26.393409   \n",
       "\n",
       "               0          0  \n",
       "0     -25.936504 -25.961329  \n",
       "1     -26.120345 -26.043134  \n",
       "2     -25.801612 -25.885519  \n",
       "3     -25.607190 -25.615177  \n",
       "4     -25.622568 -25.694433  \n",
       "...          ...        ...  \n",
       "39603 -26.272776 -26.287771  \n",
       "39604 -26.248674 -26.273253  \n",
       "39605 -26.480849 -26.499461  \n",
       "39606 -26.268088 -26.305381  \n",
       "39607 -26.470877 -26.445254  \n",
       "\n",
       "[39608 rows x 14 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pd.to_csv('data/end_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoMrNNFK8u00"
   },
   "source": [
    "##NGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97ITzGb_8ydY"
   },
   "outputs": [],
   "source": [
    "space_ngboost = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'natural_gradient': hp.choice('natural_gradient', [True, False]),\n",
    "    'col_sample': hp.quniform('col_sample', 0, 1, 0.01),\n",
    "    'minibatch_frac': hp.quniform('minibatch_frac', 0, 1, 0.01),\n",
    "    'tol': hp.uniform('tol', 1e-6, 3e-4),\n",
    "}\n",
    "\n",
    "best = fmin(fn = ngbr_objective,\n",
    "            space = space_ngboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 100)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlA8SXYc9_5t"
   },
   "outputs": [],
   "source": [
    "def get_stacking_base_datasets(model, train_x, train_y, col,test, params):\n",
    "    kf = KFold(n_splits=Config.cv, shuffle=False)\n",
    "    train_fold_pred = np.zeros((train_x.shape[0],1))\n",
    "    test_pred = np.zeros((test.shape[0],Config.cv))\n",
    "    \n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "        print('Fold : ', folder_counter, ' Start')\n",
    "        X_tr = train_x.loc[train_index]\n",
    "        y_tr = train_y[col].loc[train_index]\n",
    "        X_te = train_x.loc[valid_index] \n",
    "        \n",
    "        if model == 'cat':\n",
    "          model = CatBoostRegressor(random_state=1,\n",
    "                                    **params)\n",
    "        \n",
    "        elif model == 'extra':\n",
    "          model = ExtraTreesRegressor(random_state=1, \n",
    "                                      **params)\n",
    "\n",
    "        elif model == 'ngbr':\n",
    "          model = NGBRegressor(random_state = 1)\n",
    "        \n",
    "        elif model == 'lgbm':\n",
    "          model = LGBMRegressor(random_state=1, n_jobs=-1, \n",
    "                                **params)\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) \n",
    "        test_pred[:, folder_counter] = model.predict(test) \n",
    "        \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
    "    \n",
    "    return train_fold_pred, test_pred_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOps13z9-StG"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor(\n",
    "        n_jobs = -1,\n",
    "        random_state = 1,\n",
    "        verbose = 100,\n",
    "        **best\n",
    "    )\n",
    "\n",
    "\n",
    "# model 8개\n",
    "xx_train, xx_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n",
    "yy_train, yy_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n",
    "zz_train, zz_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n",
    "qq_train, qq_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train,yy_train,zz_train,qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test,yy_test,zz_test,qq_test), axis=1)\n",
    "\n",
    "# final_model 선택해야함\n",
    "final_model = None\n",
    "\bfinal_model.fit(Stack_final_X_train, y_train)\n",
    "stack_final = \bfinal_model.predict(Stack_final_X_test) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSh84dg_WnrV"
   },
   "outputs": [],
   "source": [
    "## col1 col2 지정\n",
    "stack_final.to_csv(f'{col1}_{col2}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FINAL_ver2.ipynb",
   "provenance": [
    {
     "file_id": "1ePNa_9qy1p14-5RE8p1iFjC8uVmR5Xqa",
     "timestamp": 1661266663713
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
